{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a740e6a",
   "metadata": {},
   "source": [
    "**Spot-checking algorithms** is a technique in applied machine learning designed to quickly and objectively provide a first set of results on a new predictive modeling problem.\n",
    "\n",
    "<br>\n",
    "\n",
    "Unlike grid searching and other types of algorithm tuning that seek the optimal algorithm or optimal configuration for an algorithm, spot-checking is intended to evaluate a diverse set of algorithms rapidly and provide a rough first-cut result. This first cut result may be used to get an idea if a problem or problem representation is indeed predictable, and if so, the types of algorithms that may be worth investigating further for the problem.\n",
    "\n",
    "<br>\n",
    "\n",
    "Spot-checking is an approach to help overcome the “hard problem” of applied machine learning and encourage you to clearly think about the higher-order search problem being performed in any machine learning project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b783722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cfa953",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "114b881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get multiple outputs in the same cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "## Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44de4380",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display all rows and columns of a dataframe instead of a truncated version\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c637405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the dataset\n",
    "\n",
    "df = pd.read_csv('data/Churn_Modeling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b61006d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating out different columns into various categories as defined above\n",
    "\n",
    "target_var = ['Exited']\n",
    "cols_to_remove = ['RowNumber', 'CustomerId']\n",
    "num_feats = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "cat_feats = ['Surname', 'Geography', 'Gender', 'HasCrCard', 'IsActiveMember']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a1066a",
   "metadata": {},
   "source": [
    "## Separating out train-test-valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de36db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separating out target variable and removing the non-essential columns\n",
    "y = df[target_var].values\n",
    "df.drop(cols_to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9256e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keeping aside a test/holdout set\n",
    "df_train_val, df_test, y_train_val, y_test = train_test_split(df, y.ravel(), test_size = 0.1, random_state = 42)\n",
    "\n",
    "## Splitting into train and validation set\n",
    "df_train, df_val, y_train, y_val = train_test_split(df_train_val, y_train_val, test_size = 0.12, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd37a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7920, 12), (1080, 12), (1000, 12), (7920,), (1080,), (1000,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.20303030303030303, 0.22037037037037038, 0.191)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape, y_train.shape, y_val.shape, y_test.shape\n",
    "np.mean(y_train), np.mean(y_val), np.mean(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a487d70",
   "metadata": {},
   "source": [
    "## Spot-checking various ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57262b7c",
   "metadata": {},
   "source": [
    "__Steps__ :\n",
    "\n",
    "- Automate data preparation and model run through Pipelines\n",
    "\n",
    "- Model Zoo : List of all models to compare/spot-check\n",
    "\n",
    "- Evaluate using k-fold Cross validation framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359d45d",
   "metadata": {},
   "source": [
    "### Automating data preparation and model run through Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f100fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "850c9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" \n",
    "    Encodes categorical columns using LabelEncoding, OneHotEncoding and TargetEncoding.\n",
    "    LabelEncoding is used for binary categorical columns\n",
    "    OneHotEncoding is used for columns with <= 10 distinct values\n",
    "    TargetEncoding is used for columns with higher cardinality (>10 distinct values)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cols = None, lcols = None, ohecols = None, tcols = None, reduce_df = False):\n",
    "        \"\"\"\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        cols : list of str\n",
    "            Columns to encode.  Default is to one-hot/target/label encode all categorical columns in the DataFrame.\n",
    "        reduce_df : bool\n",
    "            Whether to use reduced degrees of freedom for encoding\n",
    "            (that is, add N-1 one-hot columns for a column with N \n",
    "            categories). E.g. for a column with categories A, B, \n",
    "            and C: When reduce_df is True, A=[1, 0], B=[0, 1],\n",
    "            and C=[0, 0].  When reduce_df is False, A=[1, 0, 0], \n",
    "            B=[0, 1, 0], and C=[0, 0, 1]\n",
    "            Default = False\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if isinstance(cols,str):\n",
    "            self.cols = [cols]\n",
    "        else :\n",
    "            self.cols = cols\n",
    "        \n",
    "        if isinstance(lcols,str):\n",
    "            self.lcols = [lcols]\n",
    "        else :\n",
    "            self.lcols = lcols\n",
    "        \n",
    "        if isinstance(ohecols,str):\n",
    "            self.ohecols = [ohecols]\n",
    "        else :\n",
    "            self.ohecols = ohecols\n",
    "        \n",
    "        if isinstance(tcols,str):\n",
    "            self.tcols = [tcols]\n",
    "        else :\n",
    "            self.tcols = tcols\n",
    "        \n",
    "        self.reduce_df = reduce_df\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit label/one-hot/target encoder to X and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : encoder\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Encode all categorical cols by default\n",
    "        if self.cols is None:\n",
    "            self.cols = [c for c in X if str(X[c].dtype)=='object']\n",
    "\n",
    "        # Check columns are in X\n",
    "        for col in self.cols:\n",
    "            if col not in X:\n",
    "                raise ValueError('Column \\''+col+'\\' not in X')\n",
    "        \n",
    "        # Separating out lcols, ohecols and tcols\n",
    "        if self.lcols is None:\n",
    "            self.lcols = [c for c in self.cols if X[c].nunique() <= 2]\n",
    "        \n",
    "        if self.ohecols is None:\n",
    "            self.ohecols = [c for c in self.cols if ((X[c].nunique() > 2) & (X[c].nunique() <= 10))]\n",
    "        \n",
    "        if self.tcols is None:\n",
    "            self.tcols = [c for c in self.cols if X[c].nunique() > 10]\n",
    "        \n",
    "        \n",
    "        ## Create Label Encoding mapping\n",
    "        self.lmaps = dict()\n",
    "        for col in self.lcols:\n",
    "            self.lmaps[col] = dict(zip(X[col].values, X[col].astype('category').cat.codes.values))\n",
    "        \n",
    "        \n",
    "        ## Create OneHot Encoding mapping\n",
    "        self.ohemaps = dict() #dict to store map for each column\n",
    "        for col in self.ohecols:\n",
    "            self.ohemaps[col] = []\n",
    "            uniques = X[col].unique()\n",
    "            for unique in uniques:\n",
    "                self.ohemaps[col].append(unique)\n",
    "            if self.reduce_df:\n",
    "                del self.ohemaps[col][-1]\n",
    "        \n",
    "        \n",
    "        ## Create Target Encoding mapping\n",
    "        self.global_target_mean = y.mean().round(2)\n",
    "        self.sum_count = dict()\n",
    "        for col in self.tcols:\n",
    "            self.sum_count[col] = dict()\n",
    "            uniques = X[col].unique()\n",
    "            for unique in uniques:\n",
    "                ix = X[col]==unique\n",
    "                self.sum_count[col][unique] = (y[ix].sum(),ix.sum())\n",
    "        \n",
    "        \n",
    "        ## Return the fit object\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"Perform label/one-hot/target encoding transformation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to label encode\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        \n",
    "        Xo = X.copy()\n",
    "        ## Perform label encoding transformation\n",
    "        for col, lmap in self.lmaps.items():\n",
    "            \n",
    "            # Map the column\n",
    "            Xo[col] = Xo[col].map(lmap)\n",
    "            Xo[col].fillna(-1, inplace=True) ## Filling new values with -1\n",
    "        \n",
    "        \n",
    "        ## Perform one-hot encoding transformation\n",
    "        for col, vals in self.ohemaps.items():\n",
    "            for val in vals:\n",
    "                new_col = col+'_'+str(val)\n",
    "                Xo[new_col] = (Xo[col]==val).astype('uint8')\n",
    "            del Xo[col]\n",
    "        \n",
    "        \n",
    "        ## Perform LOO target encoding transformation\n",
    "        # Use normal target encoding if this is test data\n",
    "        if y is None:\n",
    "            for col in self.sum_count:\n",
    "                vals = np.full(X.shape[0], np.nan)\n",
    "                for cat, sum_count in self.sum_count[col].items():\n",
    "                    vals[X[col]==cat] = (sum_count[0]/sum_count[1]).round(2)\n",
    "                Xo[col] = vals\n",
    "                Xo[col].fillna(self.global_target_mean, inplace=True) # Filling new values by global target mean\n",
    "\n",
    "        # LOO target encode each column\n",
    "        else:\n",
    "            for col in self.sum_count:\n",
    "                vals = np.full(X.shape[0], np.nan)\n",
    "                for cat, sum_count in self.sum_count[col].items():\n",
    "                    ix = X[col]==cat\n",
    "                    if sum_count[1] > 1:\n",
    "                        vals[ix] = ((sum_count[0]-y[ix].reshape(-1,))/(sum_count[1]-1)).round(2)\n",
    "                    else :\n",
    "                        vals[ix] = ((y.sum() - y[ix])/(X.shape[0] - 1)).round(2) # Catering to the case where a particular \n",
    "                                                                                 # category level occurs only once in the dataset\n",
    "                \n",
    "                Xo[col] = vals\n",
    "                Xo[col].fillna(self.global_target_mean, inplace=True) # Filling new values by global target mean\n",
    "        \n",
    "        \n",
    "        ## Return encoded DataFrame\n",
    "        return Xo\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Fit and transform the data via label/one-hot/target encoding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to encode\n",
    "        y : pandas Series, shape = [n_samples]\n",
    "            Target values (required!).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            Input DataFrame with transformed columns\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.fit(X, y).transform(X, y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e3f5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddFeatures(BaseEstimator):\n",
    "    \"\"\"\n",
    "    Add new, engineered features using original categorical and numerical features of the DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps = 1e-6):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        eps : A small value to avoid divide by zero error. Default value is 0.000001\n",
    "        \"\"\"\n",
    "        \n",
    "        self.eps = eps\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing base columns using which new interaction-based features can be engineered\n",
    "        \"\"\"\n",
    "        Xo = X.copy()\n",
    "        ## Add 4 new columns - bal_per_product, bal_by_est_salary, tenure_age_ratio, age_surname_mean_churn\n",
    "        Xo['bal_per_product'] = Xo.Balance/(Xo.NumOfProducts + self.eps)\n",
    "        Xo['bal_by_est_salary'] = Xo.Balance/(Xo.EstimatedSalary + self.eps)\n",
    "        Xo['tenure_age_ratio'] = Xo.Tenure/(Xo.Age + self.eps)\n",
    "        Xo['age_surname_enc'] = np.sqrt(Xo.Age) * Xo.Surname\n",
    "        \n",
    "        ## Returning the updated dataframe\n",
    "        return Xo\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing base columns using which new interaction-based features can be engineered\n",
    "        \"\"\"\n",
    "        return self.fit(X,y).transform(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5687fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A custom standard scaler class with the ability to apply scaling on selected columns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scale_cols = None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        scale_cols : list of str\n",
    "            Columns on which to perform scaling and normalization. Default is to scale all numerical columns\n",
    "        \n",
    "        \"\"\"\n",
    "        self.scale_cols = scale_cols\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to scale\n",
    "        \"\"\"\n",
    "        \n",
    "        # Scaling all non-categorical columns if user doesn't provide the list of columns to scale\n",
    "        if self.scale_cols is None:\n",
    "            self.scale_cols = [c for c in X if ((str(X[c].dtype).find('float') != -1) or (str(X[c].dtype).find('int') != -1))]\n",
    "        \n",
    "     \n",
    "        ## Create mapping corresponding to scaling and normalization\n",
    "        self.maps = dict()\n",
    "        for col in self.scale_cols:\n",
    "            self.maps[col] = dict()\n",
    "            self.maps[col]['mean'] = np.mean(X[col].values).round(2)\n",
    "            self.maps[col]['std_dev'] = np.std(X[col].values).round(2)\n",
    "        \n",
    "        # Return fit object\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to scale\n",
    "        \"\"\"\n",
    "        Xo = X.copy()\n",
    "        \n",
    "        ## Map transformation to respective columns\n",
    "        for col in self.scale_cols:\n",
    "            Xo[col] = (Xo[col] - self.maps[col]['mean']) / self.maps[col]['std_dev']\n",
    "        \n",
    "        \n",
    "        # Return scaled and normalized DataFrame\n",
    "        return Xo\n",
    "    \n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
    "            DataFrame containing columns to scale\n",
    "        \"\"\"\n",
    "        # Fit and return transformed dataframe\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ff2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a57fd481",
   "metadata": {},
   "source": [
    "### Pipeline for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b27109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "## Importing relevant metrics\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede5a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns = ['Exited'], axis = 1)\n",
    "X_val = df_val.drop(columns = ['Exited'], axis = 1)\n",
    "\n",
    "cols_to_scale = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'bal_per_product', 'bal_by_est_salary', 'tenure_age_ratio'\n",
    "                ,'age_surname_enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "964a22a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dict = {0 : 1.0, 1 : 3.92}\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion = 'entropy', class_weight = weights_dict, max_depth = 4, max_features = None\n",
    "                            , min_samples_split = 25, min_samples_leaf = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07ebf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps = [('categorical_encoding', CategoricalEncoder()),\n",
    "                          ('add_new_features', AddFeatures()),\n",
    "                          ('standard_scaling', CustomScaler(cols_to_scale)),\n",
    "                          ('classifier', clf)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90497a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('categorical_encoding',\n",
       "                 CategoricalEncoder(cols=['Surname', 'Geography', 'Gender'],\n",
       "                                    lcols=['Gender'], ohecols=['Geography'],\n",
       "                                    tcols=['Surname'])),\n",
       "                ('add_new_features', AddFeatures()),\n",
       "                ('standard_scaling',\n",
       "                 CustomScaler(scale_cols=['CreditScore', 'Age', 'Balance',\n",
       "                                          'EstimatedSalary', 'bal_per_product',\n",
       "                                          'bal_by_est_salary',\n",
       "                                          'tenure_age_ratio',\n",
       "                                          'age_surname_enc'])),\n",
       "                ('classifier',\n",
       "                 DecisionTreeClassifier(class_weight={0: 1.0, 1: 3.92},\n",
       "                                        criterion='entropy', max_depth=4,\n",
       "                                        min_samples_leaf=15,\n",
       "                                        min_samples_split=25))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit pipeline with training data\n",
    "model.fit(X,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89e2d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict target values on val data\n",
    "val_preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "177113bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7477394758378411"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7436974789915967"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[633, 209],\n",
       "       [ 61, 177]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.75      0.82       842\n",
      "           1       0.46      0.74      0.57       238\n",
      "\n",
      "    accuracy                           0.75      1080\n",
      "   macro avg       0.69      0.75      0.70      1080\n",
      "weighted avg       0.81      0.75      0.77      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Validation metrics\n",
    "roc_auc_score(y_val, val_preds)\n",
    "recall_score(y_val, val_preds)\n",
    "confusion_matrix(y_val, val_preds)\n",
    "print(classification_report(y_val, val_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f64956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc25cc9f",
   "metadata": {},
   "source": [
    "## Model Zoo + k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97dbed",
   "metadata": {},
   "source": [
    "Models : RF, LGBM, XGB, Naive Bayes (Gaussian/Multinomial), kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffbc4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ede3190e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.93"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preparing data and a few common model parameters\n",
    "X = df_train.drop(columns = ['Exited'], axis = 1)\n",
    "y = y_train.ravel()\n",
    "\n",
    "weights_dict = {0 : 1.0, 1 : 3.93}\n",
    "_, num_samples = np.unique(y_train, return_counts = True)\n",
    "weight = (num_samples[0]/num_samples[1]).round(2)\n",
    "weight\n",
    "\n",
    "cols_to_scale = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'bal_per_product', 'bal_by_est_salary', 'tenure_age_ratio'\n",
    "                ,'age_surname_enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4837855",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the models to be tried out\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a989ad6",
   "metadata": {},
   "source": [
    "Read more about XGB parameters from here  : https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "\n",
    "Tips to tune parameters for LightGBM : https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b49444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing a list of models to try out in the spot-checking process\n",
    "def model_zoo(models = dict()):\n",
    "    # Tree models\n",
    "    for n_trees in [21, 1001]:\n",
    "        models['rf_' + str(n_trees)] = RandomForestClassifier(n_estimators = n_trees, n_jobs = -1, criterion = 'entropy'\n",
    "                                                              , class_weight = weights_dict, max_depth = 6, max_features = 0.6\n",
    "                                                              , min_samples_split = 30, min_samples_leaf = 20)\n",
    "        \n",
    "        models['lgb_' + str(n_trees)] = LGBMClassifier(boosting_type='dart', num_leaves=31, max_depth= 6, learning_rate=0.1\n",
    "                                                       , n_estimators=n_trees, class_weight=weights_dict, min_child_samples=20\n",
    "                                                       , colsample_bytree=0.6, reg_alpha=0.3, reg_lambda=1.0, n_jobs=- 1\n",
    "                                                       , importance_type = 'gain')\n",
    "        \n",
    "        models['xgb_' + str(n_trees)] = XGBClassifier(objective='binary:logistic', n_estimators = n_trees, max_depth = 6\n",
    "                                                      , learning_rate = 0.03, n_jobs = -1, colsample_bytree = 0.6\n",
    "                                                      , reg_alpha = 0.3, reg_lambda = 0.1, scale_pos_weight = weight)\n",
    "        \n",
    "        models['et_' + str(n_trees)] = ExtraTreesClassifier(n_estimators=n_trees, criterion = 'entropy', max_depth = 6\n",
    "                                                            , max_features = 0.6, n_jobs = -1, class_weight = weights_dict\n",
    "                                                            , min_samples_split = 30, min_samples_leaf = 20)\n",
    "    \n",
    "    # kNN models\n",
    "    for n in [3,5,11]:\n",
    "        models['knn_' + str(n)] = KNeighborsClassifier(n_neighbors=n)\n",
    "    \n",
    "    # Naive-Bayes models\n",
    "    models['gauss_nb'] = GaussianNB()\n",
    "    models['multi_nb'] = MultinomialNB()\n",
    "    models['compl_nb'] = ComplementNB()\n",
    "    models['bern_nb'] = BernoulliNB()\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90416fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Automation of data preparation and model run through pipelines\n",
    "def make_pipeline(model):\n",
    "    '''\n",
    "    Creates pipeline for the model passed as the argument. Uses standard scaling only in case of kNN models. \n",
    "    Ignores scaling step for tree/Naive Bayes models\n",
    "    '''\n",
    "    \n",
    "    if (str(model).find('KNeighborsClassifier') != -1):\n",
    "        pipe =  Pipeline(steps = [('categorical_encoding', CategoricalEncoder()),\n",
    "                              ('add_new_features', AddFeatures()),\n",
    "                              ('standard_scaling', CustomScaler(cols_to_scale)),\n",
    "                              ('classifier', model)\n",
    "                             ])\n",
    "    else :\n",
    "        pipe =  Pipeline(steps = [('categorical_encoding', CategoricalEncoder()),\n",
    "                              ('add_new_features', AddFeatures()),\n",
    "                              ('classifier', model)\n",
    "                             ])\n",
    "    \n",
    "    \n",
    "    return pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34abf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run/Evaluate all 15 models using KFold cross-validation (5 folds)\n",
    "def evaluate_models(X, y, models, folds = 5, metric = 'recall'):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # Evaluate model through automated pipelines\n",
    "        pipeline = make_pipeline(model)\n",
    "        scores = cross_val_score(pipeline, X, y, cv = folds, scoring = metric, n_jobs = -1)\n",
    "        \n",
    "        # Store results of the evaluated model\n",
    "        results[name] = scores\n",
    "        mu, sigma = np.mean(scores), np.std(scores)\n",
    "        # Printing individual model results\n",
    "        print('Model {}: mean = {}, std_dev = {}'.format(name, mu, sigma))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3476966d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall metric\n",
      "Model rf_21: mean = 0.7319537160658657, std_dev = 0.009831521899392636\n",
      "Model lgb_21: mean = 0.7462529749811342, std_dev = 0.021451811890045795\n",
      "Model xgb_21: mean = 0.7462549099282134, std_dev = 0.014095254245960575\n",
      "Model et_21: mean = 0.740035990015673, std_dev = 0.011196210753874902\n",
      "Model rf_1001: mean = 0.7381707010313268, std_dev = 0.017934078062792385\n",
      "Model lgb_1001: mean = 0.6262340124997581, std_dev = 0.02347185271419394\n",
      "Model xgb_1001: mean = 0.6125655463323078, std_dev = 0.021372739978790304\n",
      "Model et_1001: mean = 0.738182310713802, std_dev = 0.006089985807240707\n",
      "Model knn_3: mean = 0.3308391865482479, std_dev = 0.0257896061929066\n",
      "Model knn_5: mean = 0.30969795476093726, std_dev = 0.01633554361744493\n",
      "Model knn_11: mean = 0.2506143456976452, std_dev = 0.007246171430455652\n",
      "Model gauss_nb: mean = 0.03484646194926569, std_dev = 0.015468568191425867\n",
      "Model multi_nb: mean = 0.5404191095373541, std_dev = 0.022285871235774777\n",
      "Model compl_nb: mean = 0.5404191095373541, std_dev = 0.022285871235774777\n",
      "Model bern_nb: mean = 0.3065730152280335, std_dev = 0.022710670132026465\n",
      "F1-score metric\n",
      "Model rf_21: mean = 0.5966795931498978, std_dev = 0.0143468242755\n",
      "Model lgb_21: mean = 0.592902890094053, std_dev = 0.009161633119074164\n",
      "Model xgb_21: mean = 0.5962278916729552, std_dev = 0.007549183596403608\n",
      "Model et_21: mean = 0.5943043549974272, std_dev = 0.008957312107090089\n",
      "Model rf_1001: mean = 0.5972058854618447, std_dev = 0.010528669652745523\n",
      "Model lgb_1001: mean = 0.6070393974343331, std_dev = 0.01765690955763956\n",
      "Model xgb_1001: mean = 0.6116311501661631, std_dev = 0.02075479729530031\n",
      "Model et_1001: mean = 0.5936257196566732, std_dev = 0.00799319531913159\n",
      "Model knn_3: mean = 0.42030087566989466, std_dev = 0.030417310581367648\n",
      "Model knn_5: mean = 0.4128174354549231, std_dev = 0.017946203515732076\n",
      "Model knn_11: mean = 0.3681743479512917, std_dev = 0.007513684261704708\n",
      "Model gauss_nb: mean = 0.061320836385407564, std_dev = 0.025280231091869543\n",
      "Model multi_nb: mean = 0.329272413622277, std_dev = 0.011346796699221388\n",
      "Model compl_nb: mean = 0.329272413622277, std_dev = 0.011346796699221388\n",
      "Model bern_nb: mean = 0.33971572128030253, std_dev = 0.016707761800150957\n"
     ]
    }
   ],
   "source": [
    "## Spot-checking in action\n",
    "models = model_zoo()\n",
    "print('Recall metric')\n",
    "results = evaluate_models(X, y , models, metric = 'recall')\n",
    "print('F1-score metric')\n",
    "results = evaluate_models(X, y , models, metric = 'f1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84fbff9",
   "metadata": {},
   "source": [
    "Based on the relevant metric, a suitable model can be chosen for further hyperparameter tuning.\n",
    "\n",
    "LightGBM is chosen for further hyperparameter tuning because it has the best performance on recall metric and it came close second when comparing using F1-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aed395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad8bab3b",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98be618",
   "metadata": {},
   "source": [
    "RandomSearchCV vs GridSearchCV\n",
    "\n",
    "- Random Search is more suitable for large datasets, with a large number of parameter settings\n",
    "- Grid Search results in a more precise hyperparameter tuning, thus resulting in better model performance. Intelligent tuning mechanism can also help reduce the time taken in GridSearch by a large factor\n",
    "\n",
    "- Will optimize on F1 metric. We could easily reach 75% Recall from the default parameters as seen earlier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c33fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88b08830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7920, 11), (7920,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((1080, 11), (1080,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preparing data and a few common model parameters\n",
    "# Unscaled features will be used since it's a tree model\n",
    "\n",
    "X_train = df_train.drop(columns = ['Exited'], axis = 1)\n",
    "X_val = df_val.drop(columns = ['Exited'], axis = 1)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7130d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb = LGBMClassifier(boosting_type = 'dart', min_child_samples = 20, n_jobs = - 1, importance_type = 'gain', num_leaves = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ade2937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps = [('categorical_encoding', CategoricalEncoder()),\n",
    "                          ('add_new_features', AddFeatures()),\n",
    "                          ('classifier', lgb)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e416d",
   "metadata": {},
   "source": [
    "#### Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52c0c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exhaustive list of parameters\n",
    "parameters = {'classifier__n_estimators':[10, 21, 51, 100, 201, 350, 501]\n",
    "             ,'classifier__max_depth': [3, 4, 6, 9]\n",
    "             ,'classifier__num_leaves':[7, 15, 31] \n",
    "             ,'classifier__learning_rate': [0.03, 0.05, 0.1, 0.5, 1]\n",
    "             ,'classifier__colsample_bytree': [0.3, 0.6, 0.8]\n",
    "             ,'classifier__reg_alpha': [0, 0.3, 1, 5]\n",
    "             ,'classifier__reg_lambda': [0.1, 0.5, 1, 5, 10]\n",
    "             ,'classifier__class_weight': [{0:1,1:1.0}, {0:1,1:1.96}, {0:1,1:3.0}, {0:1,1:3.93}]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "350a1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = RandomizedSearchCV(model, parameters, n_iter = 20, cv = 5, scoring = 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cad15190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('categorical_encoding',\n",
       "                                              CategoricalEncoder()),\n",
       "                                             ('add_new_features',\n",
       "                                              AddFeatures()),\n",
       "                                             ('classifier',\n",
       "                                              LGBMClassifier(boosting_type='dart',\n",
       "                                                             importance_type='gain'))]),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={'classifier__class_weight': [{0: 1,\n",
       "                                                                      1: 1.0},\n",
       "                                                                     {0: 1,\n",
       "                                                                      1: 1.96},\n",
       "                                                                     {0: 1,\n",
       "                                                                      1: 3.0},\n",
       "                                                                     {0: 1,\n",
       "                                                                      1: 3.93}],\n",
       "                                        'classifier__colsample_bytree': [0.3,\n",
       "                                                                         0.6,\n",
       "                                                                         0.8],\n",
       "                                        'classifier__learning_rate': [0.03,\n",
       "                                                                      0.05, 0.1,\n",
       "                                                                      0.5, 1],\n",
       "                                        'classifier__max_depth': [3, 4, 6, 9],\n",
       "                                        'classifier__n_estimators': [10, 21, 51,\n",
       "                                                                     100, 201,\n",
       "                                                                     350, 501],\n",
       "                                        'classifier__num_leaves': [7, 15, 31],\n",
       "                                        'classifier__reg_alpha': [0, 0.3, 1, 5],\n",
       "                                        'classifier__reg_lambda': [0.1, 0.5, 1,\n",
       "                                                                   5, 10]},\n",
       "                   scoring='f1')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e289c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__reg_lambda': 1,\n",
       " 'classifier__reg_alpha': 1,\n",
       " 'classifier__num_leaves': 31,\n",
       " 'classifier__n_estimators': 350,\n",
       " 'classifier__max_depth': 9,\n",
       " 'classifier__learning_rate': 0.1,\n",
       " 'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__class_weight': {0: 1, 1: 3.0}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6222658125858743"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "867b1820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.91852312, 1.78708429, 1.7283668 , 2.19259849, 1.81131868,\n",
       "        1.76797976, 1.74927402, 1.73195825, 1.73599715, 2.16181068,\n",
       "        1.88791108, 2.03256569, 1.70244827, 1.72443695, 1.82583971,\n",
       "        2.208744  , 1.79518523, 1.72336311, 1.93083777, 1.70005465]),\n",
       " 'std_fit_time': array([0.02231485, 0.00742684, 0.00466925, 0.02684176, 0.01512076,\n",
       "        0.0087573 , 0.0146917 , 0.01695662, 0.01454215, 0.01316106,\n",
       "        0.01021788, 0.02587673, 0.01208376, 0.0086988 , 0.00741948,\n",
       "        0.01748559, 0.00749822, 0.00962909, 0.01289628, 0.00934724]),\n",
       " 'mean_score_time': array([0.36737399, 0.36422071, 0.36173139, 0.37045107, 0.36686497,\n",
       "        0.36465478, 0.36368437, 0.36521964, 0.36873617, 0.36666012,\n",
       "        0.36944184, 0.36382732, 0.35983806, 0.36204162, 0.36492181,\n",
       "        0.36897392, 0.36421437, 0.36003799, 0.3652236 , 0.36282988]),\n",
       " 'std_score_time': array([0.00173007, 0.00219587, 0.00190261, 0.00573904, 0.00336131,\n",
       "        0.00270332, 0.00350002, 0.00165539, 0.00421478, 0.00190347,\n",
       "        0.00372177, 0.00116305, 0.00291802, 0.00327766, 0.00172665,\n",
       "        0.00733763, 0.00307221, 0.00289127, 0.0038574 , 0.00692119]),\n",
       " 'param_classifier__reg_lambda': masked_array(data=[10, 0.5, 5, 5, 1, 0.1, 1, 0.5, 10, 1, 10, 0.1, 10, 1,\n",
       "                    10, 0.1, 1, 0.5, 0.5, 1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__reg_alpha': masked_array(data=[5, 0, 5, 0.3, 5, 0.3, 0.3, 0, 1, 1, 1, 0.3, 1, 5, 0.3,\n",
       "                    1, 5, 0, 1, 0],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__num_leaves': masked_array(data=[7, 7, 15, 31, 7, 15, 7, 7, 7, 31, 7, 15, 7, 15, 31, 15,\n",
       "                    15, 7, 31, 31],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__n_estimators': masked_array(data=[350, 201, 51, 350, 201, 100, 100, 21, 10, 350, 350,\n",
       "                    501, 21, 10, 201, 501, 201, 100, 350, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__max_depth': masked_array(data=[6, 6, 3, 9, 3, 6, 6, 9, 4, 9, 6, 3, 3, 6, 4, 9, 3, 6,\n",
       "                    4, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__learning_rate': masked_array(data=[0.5, 1, 1, 1, 0.1, 0.1, 1, 1, 0.1, 0.1, 0.5, 0.03, 0.5,\n",
       "                    0.5, 0.5, 0.1, 0.5, 0.03, 0.5, 0.5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__colsample_bytree': masked_array(data=[0.3, 0.8, 0.6, 0.6, 0.6, 0.6, 0.3, 0.6, 0.6, 0.8, 0.3,\n",
       "                    0.3, 0.8, 0.6, 0.6, 0.8, 0.8, 0.6, 0.3, 0.3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_classifier__class_weight': masked_array(data=[{0: 1, 1: 3.0}, {0: 1, 1: 1.0}, {0: 1, 1: 3.0},\n",
       "                    {0: 1, 1: 3.93}, {0: 1, 1: 3.93}, {0: 1, 1: 1.0},\n",
       "                    {0: 1, 1: 3.93}, {0: 1, 1: 1.96}, {0: 1, 1: 1.96},\n",
       "                    {0: 1, 1: 3.0}, {0: 1, 1: 3.93}, {0: 1, 1: 1.96},\n",
       "                    {0: 1, 1: 3.93}, {0: 1, 1: 1.96}, {0: 1, 1: 3.93},\n",
       "                    {0: 1, 1: 3.0}, {0: 1, 1: 3.0}, {0: 1, 1: 3.0},\n",
       "                    {0: 1, 1: 1.96}, {0: 1, 1: 3.93}],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'classifier__reg_lambda': 10,\n",
       "   'classifier__reg_alpha': 5,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 350,\n",
       "   'classifier__max_depth': 6,\n",
       "   'classifier__learning_rate': 0.5,\n",
       "   'classifier__colsample_bytree': 0.3,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.0}},\n",
       "  {'classifier__reg_lambda': 0.5,\n",
       "   'classifier__reg_alpha': 0,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 201,\n",
       "   'classifier__max_depth': 6,\n",
       "   'classifier__learning_rate': 1,\n",
       "   'classifier__colsample_bytree': 0.8,\n",
       "   'classifier__class_weight': {0: 1, 1: 1.0}},\n",
       "  {'classifier__reg_lambda': 5,\n",
       "   'classifier__reg_alpha': 5,\n",
       "   'classifier__num_leaves': 15,\n",
       "   'classifier__n_estimators': 51,\n",
       "   'classifier__max_depth': 3,\n",
       "   'classifier__learning_rate': 1,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.0}},\n",
       "  {'classifier__reg_lambda': 5,\n",
       "   'classifier__reg_alpha': 0.3,\n",
       "   'classifier__num_leaves': 31,\n",
       "   'classifier__n_estimators': 350,\n",
       "   'classifier__max_depth': 9,\n",
       "   'classifier__learning_rate': 1,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.93}},\n",
       "  {'classifier__reg_lambda': 1,\n",
       "   'classifier__reg_alpha': 5,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 201,\n",
       "   'classifier__max_depth': 3,\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.93}},\n",
       "  {'classifier__reg_lambda': 0.1,\n",
       "   'classifier__reg_alpha': 0.3,\n",
       "   'classifier__num_leaves': 15,\n",
       "   'classifier__n_estimators': 100,\n",
       "   'classifier__max_depth': 6,\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 1.0}},\n",
       "  {'classifier__reg_lambda': 1,\n",
       "   'classifier__reg_alpha': 0.3,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 100,\n",
       "   'classifier__max_depth': 6,\n",
       "   'classifier__learning_rate': 1,\n",
       "   'classifier__colsample_bytree': 0.3,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.93}},\n",
       "  {'classifier__reg_lambda': 0.5,\n",
       "   'classifier__reg_alpha': 0,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 21,\n",
       "   'classifier__max_depth': 9,\n",
       "   'classifier__learning_rate': 1,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 1.96}},\n",
       "  {'classifier__reg_lambda': 10,\n",
       "   'classifier__reg_alpha': 1,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 10,\n",
       "   'classifier__max_depth': 4,\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 1.96}},\n",
       "  {'classifier__reg_lambda': 1,\n",
       "   'classifier__reg_alpha': 1,\n",
       "   'classifier__num_leaves': 31,\n",
       "   'classifier__n_estimators': 350,\n",
       "   'classifier__max_depth': 9,\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__colsample_bytree': 0.8,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.0}},\n",
       "  {'classifier__reg_lambda': 10,\n",
       "   'classifier__reg_alpha': 1,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 350,\n",
       "   'classifier__max_depth': 6,\n",
       "   'classifier__learning_rate': 0.5,\n",
       "   'classifier__colsample_bytree': 0.3,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.93}},\n",
       "  {'classifier__reg_lambda': 0.1,\n",
       "   'classifier__reg_alpha': 0.3,\n",
       "   'classifier__num_leaves': 15,\n",
       "   'classifier__n_estimators': 501,\n",
       "   'classifier__max_depth': 3,\n",
       "   'classifier__learning_rate': 0.03,\n",
       "   'classifier__colsample_bytree': 0.3,\n",
       "   'classifier__class_weight': {0: 1, 1: 1.96}},\n",
       "  {'classifier__reg_lambda': 10,\n",
       "   'classifier__reg_alpha': 1,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 21,\n",
       "   'classifier__max_depth': 3,\n",
       "   'classifier__learning_rate': 0.5,\n",
       "   'classifier__colsample_bytree': 0.8,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.93}},\n",
       "  {'classifier__reg_lambda': 1,\n",
       "   'classifier__reg_alpha': 5,\n",
       "   'classifier__num_leaves': 15,\n",
       "   'classifier__n_estimators': 10,\n",
       "   'classifier__max_depth': 6,\n",
       "   'classifier__learning_rate': 0.5,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 1.96}},\n",
       "  {'classifier__reg_lambda': 10,\n",
       "   'classifier__reg_alpha': 0.3,\n",
       "   'classifier__num_leaves': 31,\n",
       "   'classifier__n_estimators': 201,\n",
       "   'classifier__max_depth': 4,\n",
       "   'classifier__learning_rate': 0.5,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.93}},\n",
       "  {'classifier__reg_lambda': 0.1,\n",
       "   'classifier__reg_alpha': 1,\n",
       "   'classifier__num_leaves': 15,\n",
       "   'classifier__n_estimators': 501,\n",
       "   'classifier__max_depth': 9,\n",
       "   'classifier__learning_rate': 0.1,\n",
       "   'classifier__colsample_bytree': 0.8,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.0}},\n",
       "  {'classifier__reg_lambda': 1,\n",
       "   'classifier__reg_alpha': 5,\n",
       "   'classifier__num_leaves': 15,\n",
       "   'classifier__n_estimators': 201,\n",
       "   'classifier__max_depth': 3,\n",
       "   'classifier__learning_rate': 0.5,\n",
       "   'classifier__colsample_bytree': 0.8,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.0}},\n",
       "  {'classifier__reg_lambda': 0.5,\n",
       "   'classifier__reg_alpha': 0,\n",
       "   'classifier__num_leaves': 7,\n",
       "   'classifier__n_estimators': 100,\n",
       "   'classifier__max_depth': 6,\n",
       "   'classifier__learning_rate': 0.03,\n",
       "   'classifier__colsample_bytree': 0.6,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.0}},\n",
       "  {'classifier__reg_lambda': 0.5,\n",
       "   'classifier__reg_alpha': 1,\n",
       "   'classifier__num_leaves': 31,\n",
       "   'classifier__n_estimators': 350,\n",
       "   'classifier__max_depth': 4,\n",
       "   'classifier__learning_rate': 0.5,\n",
       "   'classifier__colsample_bytree': 0.3,\n",
       "   'classifier__class_weight': {0: 1, 1: 1.96}},\n",
       "  {'classifier__reg_lambda': 1,\n",
       "   'classifier__reg_alpha': 0,\n",
       "   'classifier__num_leaves': 31,\n",
       "   'classifier__n_estimators': 10,\n",
       "   'classifier__max_depth': 3,\n",
       "   'classifier__learning_rate': 0.5,\n",
       "   'classifier__colsample_bytree': 0.3,\n",
       "   'classifier__class_weight': {0: 1, 1: 3.93}}],\n",
       " 'split0_test_score': array([0.60632184, 0.56732496, 0.5982906 , 0.57654723, 0.58214748,\n",
       "        0.59405941, 0.58575198, 0.609375  , 0.50107991, 0.60856269,\n",
       "        0.61705007, 0.62267343, 0.58429858, 0.61028192, 0.59481583,\n",
       "        0.61651917, 0.6031746 , 0.58992806, 0.59643436, 0.55516014]),\n",
       " 'split1_test_score': array([0.62429379, 0.56936937, 0.62784091, 0.60413355, 0.61111111,\n",
       "        0.59187621, 0.61517976, 0.61846154, 0.55489022, 0.63690476,\n",
       "        0.61372813, 0.63973064, 0.59801489, 0.64556962, 0.62972973,\n",
       "        0.62170088, 0.6299435 , 0.61428571, 0.61812298, 0.56344587]),\n",
       " 'split2_test_score': array([0.60930889, 0.5647482 , 0.58807212, 0.5487013 , 0.59425718,\n",
       "        0.5697446 , 0.56960409, 0.60506706, 0.48625793, 0.60856269,\n",
       "        0.5913272 , 0.59246575, 0.60294118, 0.60547504, 0.60776439,\n",
       "        0.59530792, 0.59887006, 0.58309038, 0.58688525, 0.55529412]),\n",
       " 'split3_test_score': array([0.62553802, 0.58064516, 0.59862069, 0.61172742, 0.60723514,\n",
       "        0.59215686, 0.60288336, 0.61032864, 0.54658385, 0.64732824,\n",
       "        0.61872456, 0.63210702, 0.61731493, 0.62809917, 0.61872456,\n",
       "        0.62202381, 0.63464338, 0.63096961, 0.62337662, 0.57845433]),\n",
       " 'split4_test_score': array([0.60614525, 0.54054054, 0.61072902, 0.57281553, 0.60476788,\n",
       "        0.56046065, 0.58762887, 0.58345865, 0.49079755, 0.60997067,\n",
       "        0.58868895, 0.60229133, 0.61118012, 0.60763359, 0.5952381 ,\n",
       "        0.6251809 , 0.61645746, 0.5971831 , 0.57413249, 0.53501722]),\n",
       " 'mean_test_score': array([0.61432156, 0.56452565, 0.60471067, 0.58278501, 0.59990376,\n",
       "        0.58165955, 0.59220961, 0.60533818, 0.51592189, 0.62226581,\n",
       "        0.60590378, 0.61785364, 0.60274994, 0.61941187, 0.60925452,\n",
       "        0.61614654, 0.6166178 , 0.60309137, 0.59979034, 0.55747434]),\n",
       " 'std_test_score': array([0.00873188, 0.01316413, 0.01361158, 0.02277465, 0.01048983,\n",
       "        0.01385417, 0.01559006, 0.01176614, 0.02894876, 0.01654778,\n",
       "        0.01310465, 0.01784221, 0.01136765, 0.01533634, 0.01353567,\n",
       "        0.01078332, 0.01412957, 0.01738235, 0.01859316, 0.01407303]),\n",
       " 'rank_test_score': array([ 6, 18, 10, 16, 13, 17, 15,  9, 20,  1,  8,  3, 12,  2,  7,  5,  4,\n",
       "        11, 14, 19])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50e4f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bdf1dad",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5e2ab303",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Current list of parameters\n",
    "parameters = {'classifier__n_estimators':[201]\n",
    "             ,'classifier__max_depth': [6]\n",
    "             ,'classifier__num_leaves': [63]\n",
    "             ,'classifier__learning_rate': [0.1]\n",
    "             ,'classifier__colsample_bytree': [0.6, 0.8]\n",
    "             ,'classifier__reg_alpha': [0, 1, 10]\n",
    "             ,'classifier__reg_lambda': [0.1, 1, 5]\n",
    "             ,'classifier__class_weight': [{0:1,1:3.0}]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9e17d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(model, parameters, cv = 5, scoring = 'f1', n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a54bac9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('categorical_encoding',\n",
       "                                        CategoricalEncoder()),\n",
       "                                       ('add_new_features', AddFeatures()),\n",
       "                                       ('classifier',\n",
       "                                        LGBMClassifier(boosting_type='dart',\n",
       "                                                       importance_type='gain'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'classifier__class_weight': [{0: 1, 1: 3.0}],\n",
       "                         'classifier__colsample_bytree': [0.6, 0.8],\n",
       "                         'classifier__learning_rate': [0.1],\n",
       "                         'classifier__max_depth': [6],\n",
       "                         'classifier__n_estimators': [201],\n",
       "                         'classifier__num_leaves': [63],\n",
       "                         'classifier__reg_alpha': [0, 1, 10],\n",
       "                         'classifier__reg_lambda': [0.1, 1, 5]},\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67ddba37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__class_weight': {0: 1, 1: 3.0},\n",
       " 'classifier__colsample_bytree': 0.8,\n",
       " 'classifier__learning_rate': 0.1,\n",
       " 'classifier__max_depth': 6,\n",
       " 'classifier__n_estimators': 201,\n",
       " 'classifier__num_leaves': 63,\n",
       " 'classifier__reg_alpha': 0,\n",
       " 'classifier__reg_lambda': 0.1}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6259056069494722"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c10cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f1b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d014c341",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70ccf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4a12cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7920, 11), (7920,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((1080, 11), (1080,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preparing data for error analysis\n",
    "# Unscaled features will be used since it's a tree model\n",
    "\n",
    "X_train = df_train.drop(columns = ['Exited'], axis = 1)\n",
    "X_val = df_val.drop(columns = ['Exited'], axis = 1)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e92dd059",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Three versions of the final model with best params for F1-score metric\n",
    "\n",
    "# Equal weights to both target classes (no class imbalance correction)\n",
    "lgb1 = LGBMClassifier(boosting_type = 'dart', class_weight = {0: 1, 1: 1}, min_child_samples = 20, n_jobs = - 1\n",
    "                     , importance_type = 'gain', max_depth = 4, num_leaves = 31, colsample_bytree = 0.6, learning_rate = 0.1\n",
    "                     , n_estimators = 21, reg_alpha = 0, reg_lambda = 0.5)\n",
    "\n",
    "# Addressing class imbalance completely by weighting the undersampled class by the class imbalance ratio\n",
    "lgb2 = LGBMClassifier(boosting_type = 'dart', class_weight = {0: 1, 1: 3.93}, min_child_samples = 20, n_jobs = - 1\n",
    "                     , importance_type = 'gain', max_depth = 6, num_leaves = 63, colsample_bytree = 0.6, learning_rate = 0.1\n",
    "                     , n_estimators = 201, reg_alpha = 1, reg_lambda = 1)\n",
    "\n",
    "\n",
    "# Best class_weight parameter settings (partial class imbalance correction)\n",
    "lgb3 = LGBMClassifier(boosting_type = 'dart', class_weight = {0: 1, 1: 3.0}, min_child_samples = 20, n_jobs = - 1\n",
    "                     , importance_type = 'gain', max_depth = 6, num_leaves = 63, colsample_bytree = 0.6, learning_rate = 0.1\n",
    "                     , n_estimators = 201, reg_alpha = 1, reg_lambda = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce4ee3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3 different Pipeline objects for the 3 models defined above\n",
    "model_1 = Pipeline(steps = [('categorical_encoding', CategoricalEncoder()),\n",
    "                          ('add_new_features', AddFeatures()),\n",
    "                          ('classifier', lgb1)\n",
    "                         ])\n",
    "\n",
    "model_2 = Pipeline(steps = [('categorical_encoding', CategoricalEncoder()),\n",
    "                          ('add_new_features', AddFeatures()),\n",
    "                          ('classifier', lgb2)\n",
    "                         ])\n",
    "\n",
    "model_3 = Pipeline(steps = [('categorical_encoding', CategoricalEncoder()),\n",
    "                          ('add_new_features', AddFeatures()),\n",
    "                          ('classifier', lgb3)\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9aeb3da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('categorical_encoding',\n",
       "                 CategoricalEncoder(cols=['Surname', 'Geography', 'Gender'],\n",
       "                                    lcols=['Gender'], ohecols=['Geography'],\n",
       "                                    tcols=['Surname'])),\n",
       "                ('add_new_features', AddFeatures()),\n",
       "                ('classifier',\n",
       "                 LGBMClassifier(boosting_type='dart', class_weight={0: 1, 1: 1},\n",
       "                                colsample_bytree=0.6, importance_type='gain',\n",
       "                                max_depth=4, n_estimators=21, reg_alpha=0,\n",
       "                                reg_lambda=0.5))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('categorical_encoding',\n",
       "                 CategoricalEncoder(cols=['Surname', 'Geography', 'Gender'],\n",
       "                                    lcols=['Gender'], ohecols=['Geography'],\n",
       "                                    tcols=['Surname'])),\n",
       "                ('add_new_features', AddFeatures()),\n",
       "                ('classifier',\n",
       "                 LGBMClassifier(boosting_type='dart',\n",
       "                                class_weight={0: 1, 1: 3.93},\n",
       "                                colsample_bytree=0.6, importance_type='gain',\n",
       "                                max_depth=6, n_estimators=201, num_leaves=63,\n",
       "                                reg_alpha=1, reg_lambda=1))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('categorical_encoding',\n",
       "                 CategoricalEncoder(cols=['Surname', 'Geography', 'Gender'],\n",
       "                                    lcols=['Gender'], ohecols=['Geography'],\n",
       "                                    tcols=['Surname'])),\n",
       "                ('add_new_features', AddFeatures()),\n",
       "                ('classifier',\n",
       "                 LGBMClassifier(boosting_type='dart',\n",
       "                                class_weight={0: 1, 1: 3.0},\n",
       "                                colsample_bytree=0.6, importance_type='gain',\n",
       "                                max_depth=6, n_estimators=201, num_leaves=63,\n",
       "                                reg_alpha=1, reg_lambda=1))])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fitting each of these models\n",
    "model_1.fit(X_train, y_train.ravel())\n",
    "model_2.fit(X_train, y_train.ravel())\n",
    "model_3.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdaf5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting prediction probabilities from each of these models\n",
    "m1_pred_probs_trn = model_1.predict_proba(X_train)\n",
    "m2_pred_probs_trn = model_2.predict_proba(X_train)\n",
    "m3_pred_probs_trn = model_3.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "322e3b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1_pred</th>\n",
       "      <th>m2_pred</th>\n",
       "      <th>m3_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m1_pred</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907405</td>\n",
       "      <td>0.923072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m2_pred</th>\n",
       "      <td>0.907405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m3_pred</th>\n",
       "      <td>0.923072</td>\n",
       "      <td>0.992430</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          m1_pred   m2_pred   m3_pred\n",
       "m1_pred  1.000000  0.907405  0.923072\n",
       "m2_pred  0.907405  1.000000  0.992430\n",
       "m3_pred  0.923072  0.992430  1.000000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Checking correlations between the predictions of the 3 models\n",
    "df_t = pd.DataFrame({'m1_pred': m1_pred_probs_trn[:,1], 'm2_pred': m2_pred_probs_trn[:,1], 'm3_pred': m3_pred_probs_trn[:,1]})\n",
    "df_t.shape\n",
    "df_t.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c02ad",
   "metadata": {},
   "source": [
    "Although models m1 and m2 are highly correlated (0.9), they are still less closely associated than m2 and m3.\n",
    "Thus, we'll try to form an ensemble of m1 and m2 (model averaging/stacking) and see if that improves the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99ea4b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing relevant metric libraries\n",
    "from sklearn.metrics import roc_auc_score, f1_score, recall_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9e488517",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting prediction probabilities from each of these models\n",
    "m1_pred_probs_val = model_1.predict_proba(X_val)\n",
    "m2_pred_probs_val = model_2.predict_proba(X_val)\n",
    "m3_pred_probs_val = model_3.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe323f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba5b6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best model (Model 3) predictions\n",
    "m3_preds = np.where(m3_pred_probs_val[:,1] >= threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70554218",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model averaging predictions (Weighted average)\n",
    "m1_m2_preds = np.where(((0.1*m1_pred_probs_val[:,1]) + (0.9*m2_pred_probs_val[:,1])) >= threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dbb3ebce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7605840435936845"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6470588235294118"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[736, 106],\n",
       "       [ 84, 154]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.89       842\n",
      "           1       0.59      0.65      0.62       238\n",
      "\n",
      "    accuracy                           0.82      1080\n",
      "   macro avg       0.74      0.76      0.75      1080\n",
      "weighted avg       0.83      0.82      0.83      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Model 3 (Best model, tuned by GridSearch) performance on validation set\n",
    "roc_auc_score(y_val, m3_preds)\n",
    "recall_score(y_val, m3_preds)\n",
    "confusion_matrix(y_val, m3_preds)\n",
    "print(classification_report(y_val, m3_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c62b34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7705393321223977"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6764705882352942"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[728, 114],\n",
       "       [ 77, 161]], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.86      0.88       842\n",
      "           1       0.59      0.68      0.63       238\n",
      "\n",
      "    accuracy                           0.82      1080\n",
      "   macro avg       0.74      0.77      0.76      1080\n",
      "weighted avg       0.83      0.82      0.83      1080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Ensemble model prediction on validation set\n",
    "roc_auc_score(y_val, m1_m2_preds)\n",
    "recall_score(y_val, m1_m2_preds)\n",
    "confusion_matrix(y_val, m1_m2_preds)\n",
    "print(classification_report(y_val, m1_m2_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f1573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time_series",
   "language": "python",
   "name": "time_series"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
